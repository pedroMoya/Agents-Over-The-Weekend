{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyeCwDW69Zf3"
      },
      "source": [
        "# Codes for bootcamp talk: Advanced LLM & Agent Systems Bootcamp\n",
        "By: Lior Gazit.  \n",
        "Repo: [agentic_actions_locally_hosted](https://github.com/LiorGazit/agentic_actions_locally_hosted)  \n",
        "\n",
        "<a target=\"_blank\" href=\"???\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a> (pick a GPU Colab session for fastest computing)  \n",
        "\n",
        "```\n",
        "Disclaimer: The content and ideas presented in this notebook are solely those of the author, Lior Gazit, and do not represent the views or intellectual property of the author's employer.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn9qIRLS9Zf7"
      },
      "source": [
        "Installing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMpPyY829Zf9",
        "outputId": "df7759b9-a798-4998-d661-e16314755229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m367.7/367.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.2/108.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m306.4/306.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install sentence-transformers faiss-cpu langchain tiktoken langsmith langchain_openai -U \"autogen-agentchat\" \"autogen-ext[openai]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uU75SUy9ZgA"
      },
      "source": [
        "If this notebook is run outside of the repo's codes, get the necessary code from the remote repo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MVDmCWnO9ZgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3cebed-dd5c-467e-aaed-7f287aa8fbb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded spin_up_LLM.py from GitHub\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# If the module isn't already present (e.g. in Colab), fetch it from GitHub\n",
        "if not os.path.exists(\"spin_up_LLM.py\"):\n",
        "    url = \"https://raw.githubusercontent.com/LiorGazit/agentic_actions_locally_hosted/refs/heads/main/spin_up_LLM.py\"\n",
        "    resp = requests.get(url)\n",
        "    resp.raise_for_status()\n",
        "    with open(\"spin_up_LLM.py\", \"w\") as f:\n",
        "        f.write(resp.text)\n",
        "    print(\"Downloaded spin_up_LLM.py from GitHub\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtiN5hUF9ZgH"
      },
      "source": [
        "## Demo 2: Multi-Agent Team Interaction (Agent Collaboration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dr018PW9ZgH",
        "outputId": "0bf7a15e-6171-4492-862b-ac7989339f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Installing Ollama...\n",
            "ğŸš€ Starting Ollama server...\n",
            "â†’ Ollama PID: 3258\n",
            "â³ Waiting for Ollama to be readyâ€¦\n",
            "ğŸš€ Pulling model 'CodeLlama'â€¦\n",
            "Available models:\n",
            "NAME                ID              SIZE      MODIFIED               \n",
            "CodeLlama:latest    8fdf8f752f6e    3.8 GB    Less than a second ago    \n",
            "\n",
            "ğŸš€ Installing langchain-ollamaâ€¦\n",
            "ğŸš€ Starting Ollama server...\n",
            "â†’ Ollama PID: 3517\n",
            "â³ Waiting for Ollama to be readyâ€¦\n",
            "ğŸš€ Pulling model 'Llama2'â€¦\n",
            "Available models:\n",
            "NAME                ID              SIZE      MODIFIED               \n",
            "Llama2:latest       78e26419b446    3.8 GB    Less than a second ago    \n",
            "CodeLlama:latest    8fdf8f752f6e    3.8 GB    47 seconds ago            \n",
            "\n",
            "ğŸš€ Installing langchain-ollamaâ€¦\n",
            "\n",
            "\n",
            "Coder's output:\n",
            " \n",
            "def is_prime(n):\n",
            "    if n <= 1:\n",
            "        return False\n",
            "    for i in range(2, int(n ** 0.5) + 1):\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "\n",
            "Reviewer's feedback:\n",
            "  As a code reviewer, I have reviewed the provided code snippet and here are my thoughts on it:\n",
            "\n",
            "The code appears to be well-structured and easy to read. The use of functions and variables makes the code more organized and manageable. I like how the main logic is encapsulated in the `is_prime` function, and how the `done` variable is used to indicate when the function has finished executing.\n",
            "\n",
            "However, there are a few things that could be improved:\n",
            "\n",
            "1. Code organization: The code is currently indented too much, which can make it harder to read and understand. I would suggest using spaces instead of tabs for indentation, and reducing the amount of indentation overall.\n",
            "2. Variable names: Some of the variable names are a bit long and could be shortened without losing any clarity. For example, `n` could be renamed to `number`, and `done_reason` could be renamed to `stop_reason`.\n",
            "3. Comments: While the code has some comments, they could be more extensive. For example, the `if n <= 1:` block could have a comment explaining why that base case is necessary. Similarly, the `range(2, int(n ** 0.5) + 1)` line could have a comment explaining what that range represents.\n",
            "4. Performance: The code uses a loop to check if a number is prime, which can be computationally expensive for large numbers. Depending on the use case, it might be more efficient to use a different algorithm, such as the Sieve of Eratosthenes.\n",
            "\n",
            "Overall, the code appears to be well-written and easy to understand, but there are some areas where it could be improved for readability and performance. I approve the code with minor revisions.\n"
          ]
        }
      ],
      "source": [
        "from spin_up_LLM import spin_up_LLM\n",
        "# IMPORTANT: this code cell runs for ~3 minutes on a Google Colab free GPU session, but ~15 minutes in a Google Colab free CPU session!\n",
        "coder = spin_up_LLM(chosen_llm=\"CodeLlama\")  # or OpenAI model\n",
        "reviewer = spin_up_LLM(chosen_llm=\"Llama2\")  # a more general model for critique\n",
        "\n",
        "task = \"Write a Python function to check if a number is prime.\"\n",
        "conversation = []\n",
        "# Initialize conversation\n",
        "conversation.append((\"System\", \"Agents: collaborate to solve the task. Coder writes code, Reviewer suggests fixes.\"))\n",
        "conversation.append((\"User\", task))\n",
        "\n",
        "# Agent A (Coder) turn\n",
        "code_response = coder.generate([f\"Task: {task}\\nRole: Coder\\nYou are a coding agent. Provide code only.\\n\"])\n",
        "conversation.append((\"Coder\", code_response.generations[0][0].text))\n",
        "\n",
        "# Agent B (Reviewer) turn\n",
        "review_response = reviewer.generate([f\"Code:\\n{code_response}\\nRole: Reviewer\\nYou are a code reviewer. Provide feedback or approve.\\n\"])\n",
        "conversation.append((\"Reviewer\", review_response.generations[0][0].text))\n",
        "\n",
        "print(\"\\n\\nCoder's output:\\n\", code_response.generations[0][0].text)\n",
        "print(\"\\nReviewer's feedback:\\n\", review_response.generations[0][0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P4Ej8DS9ZgI"
      },
      "source": [
        "Now, here is an example using AutoGen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtutwBfT9ZgJ",
        "outputId": "fdf89790-608c-4a58-91d3-dd0dff4b5684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your OpenAI API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "=== Coderâ€™s Output ===\n",
            "\n",
            "Write a Python function `is_prime(n)` that returns True if `n` is prime.\n",
            "```python\n",
            "def is_prime(n):\n",
            "    if n <= 1:\n",
            "        return False\n",
            "    if n <= 3:\n",
            "        return True\n",
            "    if n % 2 == 0 or n % 3 == 0:\n",
            "        return False\n",
            "    i = 5\n",
            "    while i * i <= n:\n",
            "        if n % i == 0 or n % (i + 2) == 0:\n",
            "            return False\n",
            "        i += 6\n",
            "    return True\n",
            "```\n",
            "\n",
            "=== Reviewerâ€™s Feedback ===\n",
            "\n",
            "Review the following code for correctness and edge cases:\n",
            "\n",
            "messages=[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 28, 20, 57, 9, 938085, tzinfo=datetime.timezone.utc), content='Write a Python function `is_prime(n)` that returns True if `n` is prime.', type='TextMessage'), TextMessage(source='Coder', models_usage=RequestUsage(prompt_tokens=43, completion_tokens=102), metadata={}, created_at=datetime.datetime(2025, 6, 28, 20, 57, 12, 292148, tzinfo=datetime.timezone.utc), content='```python\\ndef is_prime(n):\\n    if n <= 1:\\n        return False\\n    if n <= 3:\\n        return True\\n    if n % 2 == 0 or n % 3 == 0:\\n        return False\\n    i = 5\\n    while i * i <= n:\\n        if n % i == 0 or n % (i + 2) == 0:\\n            return False\\n        i += 6\\n    return True\\n```', type='TextMessage')] stop_reason=None\n",
            "The code provided is a list of `TextMessage` objects, each containing metadata and content related to a conversation about writing a Python function `is_prime(n)`. The function `is_prime(n)` is included in the content of one of the messages. Let's review the `is_prime` function for correctness and edge cases:\n",
            "\n",
            "### Function Review\n",
            "\n",
            "```python\n",
            "def is_prime(n):\n",
            "    if n <= 1:\n",
            "        return False\n",
            "    if n <= 3:\n",
            "        return True\n",
            "    if n % 2 == 0 or n % 3 == 0:\n",
            "        return False\n",
            "    i = 5\n",
            "    while i * i <= n:\n",
            "        if n % i == 0 or n % (i + 2) == 0:\n",
            "            return False\n",
            "        i += 6\n",
            "    return True\n",
            "```\n",
            "\n",
            "### Correctness\n",
            "\n",
            "1. **Basic Checks**:\n",
            "   - The function correctly returns `False` for numbers less than or equal to 1, as these are not prime.\n",
            "   - It correctly returns `True` for numbers 2 and 3, which are prime.\n",
            "\n",
            "2. **Divisibility Checks**:\n",
            "   - The function checks divisibility by 2 and 3 early, which is efficient since these are the smallest prime numbers.\n",
            "\n",
            "3. **Loop for Larger Numbers**:\n",
            "   - The loop starts at 5 and checks divisibility for numbers of the form `6k Â± 1`, which is a common optimization for checking primality.\n",
            "   - The loop condition `i * i <= n` is correct, as a larger factor of `n` must be a multiple of a smaller factor that has already been checked.\n",
            "\n",
            "### Edge Cases\n",
            "\n",
            "1. **Negative Numbers**:\n",
            "   - The function correctly returns `False` for negative numbers, as they are not prime.\n",
            "\n",
            "2. **Zero and One**:\n",
            "   - The function correctly returns `False` for 0 and 1, as they are not prime.\n",
            "\n",
            "3. **Small Primes**:\n",
            "   - The function correctly identifies small prime numbers like 2 and 3.\n",
            "\n",
            "4. **Even Numbers Greater Than 2**:\n",
            "   - The function correctly returns `False` for even numbers greater than 2.\n",
            "\n",
            "5. **Large Numbers**:\n",
            "   - The function should handle large numbers efficiently due to the `6k Â± 1` optimization.\n",
            "\n",
            "### Potential Improvements\n",
            "\n",
            "- **Type Checking**: The function does not handle non-integer inputs. It could be improved by adding a type check to ensure `n` is an integer.\n",
            "- **Performance**: While the function is efficient for most practical purposes, for extremely large numbers, further optimizations or probabilistic methods might be considered.\n",
            "\n",
            "Overall, the `is_prime` function is implemented correctly and handles most edge cases well. It could be enhanced with input validation to ensure robustness against non-integer inputs.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import asyncio\n",
        "# In Colab, use getpass to securely prompt for your API key\n",
        "from getpass import getpass\n",
        "import openai\n",
        "\n",
        "openai.api_key = getpass(\"Paste your OpenAI API key: \")\n",
        "\n",
        "# 1. Import the agent classes and the OpenAI client\n",
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "async def multi_agent_demo():\n",
        "    # 2. Configure your OpenAI API key\n",
        "    api_key = openai.api_key\n",
        "    if not api_key:\n",
        "        openai.api_key = getpass(\"Paste your OpenAI API key: \")\n",
        "\n",
        "    # 3. Create the OpenAI model client\n",
        "    model_client = OpenAIChatCompletionClient(\n",
        "        model=\"gpt-4o\",\n",
        "        api_key=api_key,\n",
        "        temperature=0.0,\n",
        "    )\n",
        "\n",
        "    # 4. Instantiate two LLM agents with distinct roles\n",
        "    coder = AssistantAgent(\n",
        "        name=\"Coder\",\n",
        "        model_client=model_client,\n",
        "        system_message=\"You are a Python coding assistant. Produce only working code.\"\n",
        "    )\n",
        "    reviewer = AssistantAgent(\n",
        "        name=\"Reviewer\",\n",
        "        model_client=model_client,\n",
        "        system_message=\"You are a code reviewer. Point out bugs or edge cases.\"\n",
        "    )\n",
        "\n",
        "    # 5. Coder agent writes a function\n",
        "    code_task = \"Write a Python function `is_prime(n)` that returns True if `n` is prime.\"\n",
        "    code = await coder.run(task=code_task)\n",
        "    print(\"=== Coderâ€™s Output ===\\n\")\n",
        "    for msg in code.messages:\n",
        "        print(msg.content)\n",
        "\n",
        "    # 6. Reviewer agent critiques the code\n",
        "    review = await reviewer.run(task=f\"Review the following code for correctness and edge cases:\\n\\n{code}\")\n",
        "    print(\"\\n=== Reviewerâ€™s Feedback ===\\n\")\n",
        "    for msg in review.messages:\n",
        "        print(msg.content)\n",
        "\n",
        "    # 7. Clean up\n",
        "    await model_client.close()\n",
        "\n",
        "# 8. Execute the multiâ€‘agent demo\n",
        "await multi_agent_demo()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCIX_C-MVGEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}