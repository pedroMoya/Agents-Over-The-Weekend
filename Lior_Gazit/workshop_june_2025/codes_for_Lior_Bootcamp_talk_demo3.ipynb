{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyeCwDW69Zf3"
      },
      "source": [
        "# Codes for bootcamp talk: Advanced LLM & Agent Systems Bootcamp\n",
        "By: [Lior Gazit](https://github.com/LiorGazit).  \n",
        "Repo: [Agents-Over-The-Weekend](https://github.com/PacktPublishing/Agents-Over-The-Weekend/)  \n",
        "Running LLMs locally for free: This code leverages [this code](https://github.com/LiorGazit/agentic_actions_locally_hosted/blob/main/spin_up_LLM.py) that is dedicated to running open and free LLMs locally.  \n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Agents-Over-The-Weekend/blob/main/Lior_Gazit/workshop_june_2025/codes_for_Lior_Bootcamp_talk_demo3.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a> (pick a GPU Colab session for fastest computing)  \n",
        "\n",
        "```\n",
        "Disclaimer: The content and ideas presented in this notebook are solely those of the author, Lior Gazit, and do not represent the views or intellectual property of the author's employer.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn9qIRLS9Zf7"
      },
      "source": [
        "Installing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMpPyY829Zf9",
        "outputId": "20d3b57d-b9a8-40cd-cb07-112b1920264d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m367.7/367.7 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.2/108.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m306.4/306.4 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install sentence-transformers faiss-cpu langchain tiktoken langsmith langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uU75SUy9ZgA"
      },
      "source": [
        "If this notebook is run outside of the repo's codes, get the necessary code from the remote repo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVDmCWnO9ZgB",
        "outputId": "faaaec7b-4cc1-4893-986a-595eb3ed5b71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded spin_up_LLM.py from GitHub\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# If the module isn't already present (e.g. in Colab), fetch it from GitHub\n",
        "if not os.path.exists(\"spin_up_LLM.py\"):\n",
        "    url = \"https://raw.githubusercontent.com/LiorGazit/agentic_actions_locally_hosted/refs/heads/main/spin_up_LLM.py\"\n",
        "    resp = requests.get(url)\n",
        "    resp.raise_for_status()\n",
        "    with open(\"spin_up_LLM.py\", \"w\") as f:\n",
        "        f.write(resp.text)\n",
        "    print(\"Downloaded spin_up_LLM.py from GitHub\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYIgvh3f9ZgK"
      },
      "source": [
        "## Demo 3: Monitoring & Tracing Example, and Model Differences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWO5Ngf2tDKb"
      },
      "source": [
        "Code example using LangSmiths's trace support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwMTGwwuvgCY",
        "outputId": "5007ec1f-75df-460c-f97d-12c92effda4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your LangChain API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Paste your OpenAI API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "\n",
            "Starting tracing for project <multi-agent-demo04>, funtion <multi_agent_interaction>\n",
            "\n",
            "Task for coder to perform:\n",
            "Write a Python function `reverse_string(s)` that returns the reverse of the string.\n",
            "\n",
            "=== Coder Output (Time: 1.21s, Tokens: 41) ===\n",
            "\n",
            "Certainly! Here's a concise Python function to reverse a string:\n",
            "\n",
            "```python\n",
            "def reverse_string(s):\n",
            "    return s[::-1]\n",
            "```\n",
            "\n",
            "This function uses Python's slicing feature to reverse the string.\n",
            "\n",
            "=== Reviewer Feedback (Time: 3.47s, Tokens: 279) ===\n",
            "\n",
            "The provided Python function to reverse a string is both concise and efficient. It utilizes Python's slicing feature, which is a common and effective way to reverse a string. However, there are a few points to consider for improvements or additional context:\n",
            "\n",
            "1. **Type Checking**: Ensure that the input is indeed a string. If the function is called with a non-string argument, it might lead to unexpected behavior. You could add a type check to handle such cases gracefully.\n",
            "\n",
            "2. **Docstring**: Adding a docstring would improve the readability and usability of the function by explaining what it does, its parameters, and its return value.\n",
            "\n",
            "3. **Edge Cases**: Consider mentioning or handling edge cases, such as when the input string is empty or contains special characters.\n",
            "\n",
            "Here's an improved version of the function with these considerations:\n",
            "\n",
            "```python\n",
            "def reverse_string(s):\n",
            "    \"\"\"\n",
            "    Reverses the given string.\n",
            "\n",
            "    Parameters:\n",
            "    s (str): The string to be reversed.\n",
            "\n",
            "    Returns:\n",
            "    str: The reversed string.\n",
            "\n",
            "    Raises:\n",
            "    TypeError: If the input is not a string.\n",
            "    \"\"\"\n",
            "    if not isinstance(s, str):\n",
            "        raise TypeError(\"Input must be a string\")\n",
            "    return s[::-1]\n",
            "```\n",
            "\n",
            "This version includes a docstring for better documentation and a type check to ensure the input is a string, raising a `TypeError` if it is not.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import openai\n",
        "\n",
        "# Set up environment variables (make sure your keys are set correctly)\n",
        "if \"langchain_api_key\" not in globals():\n",
        "  langchain_api_key = getpass(\"Paste your LangChain API key: \")\n",
        "if not openai.api_key:\n",
        "  openai.api_key = getpass(\"Paste your OpenAI API key: \")\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"]=\"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "# IMPORTANT: If you change the designated project, you must restart the notebook kernel.\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"multi-agent-demo04\"\n",
        "\n",
        "import time\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langsmith import traceable\n",
        "from langsmith.run_helpers import trace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import tiktoken\n",
        "\n",
        "# Helper to count tokens\n",
        "def count_tokens(text, encoding_name=\"cl100k_base\"):\n",
        "    enc = tiktoken.get_encoding(encoding_name)\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "# Setup LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Prompts for agents\n",
        "coder_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a coding assistant. Write concise Python code for this task:\\n{task}\"\n",
        ")\n",
        "\n",
        "reviewer_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a meticulous code reviewer. Identify bugs or improvements in the following code:\\n{code}\"\n",
        ")\n",
        "\n",
        "# Chains\n",
        "coder_chain = coder_prompt | llm\n",
        "reviewer_chain = reviewer_prompt | llm\n",
        "\n",
        "# Traceable agent function\n",
        "@traceable(name=\"multi_agent_interaction\")\n",
        "def multi_agent_interaction(task):\n",
        "    # Coder\n",
        "    start_coder = time.time()\n",
        "    coder_response = coder_chain.invoke({\"task\": task})\n",
        "    coder_duration = time.time() - start_coder\n",
        "    coder_code = coder_response.content\n",
        "\n",
        "    print(f\"\\n=== Coder Output (Time: {coder_duration:.2f}s, Tokens: {count_tokens(coder_code)}) ===\\n\")\n",
        "    print(coder_code)\n",
        "\n",
        "    # Reviewer\n",
        "    start_reviewer = time.time()\n",
        "    reviewer_response = reviewer_chain.invoke({\"code\": coder_code})\n",
        "    reviewer_duration = time.time() - start_reviewer\n",
        "    reviewer_feedback = reviewer_response.content\n",
        "\n",
        "    print(f\"\\n=== Reviewer Feedback (Time: {reviewer_duration:.2f}s, Tokens: {count_tokens(reviewer_feedback)}) ===\\n\")\n",
        "    print(reviewer_feedback)\n",
        "\n",
        "# Execute with trace context\n",
        "with trace(\"multi_agent_demo_run\"):\n",
        "    print(\"\\nStarting tracing for project <\" + os.environ[\"LANGCHAIN_PROJECT\"] + \">, funtion <multi_agent_interaction>\")\n",
        "    task_description = \"Write a Python function `reverse_string(s)` that returns the reverse of the string.\"\n",
        "    print(f\"\\nTask for coder to perform:\\n{task_description}\")\n",
        "    multi_agent_interaction(task_description)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F11TLCs39ZgK"
      },
      "source": [
        "Code example for where building the logging process ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7EXGKqf9ZgK",
        "outputId": "532c077a-25ce-41eb-e704-5e0b4fef362c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Installing Ollama...\n",
            "ğŸš€ Starting Ollama server...\n",
            "â†’ Ollama PID: 2474\n",
            "â³ Waiting for Ollama to be readyâ€¦\n",
            "ğŸš€ Pulling model 'CodeLlama'â€¦\n",
            "Available models:\n",
            "NAME                ID              SIZE      MODIFIED               \n",
            "CodeLlama:latest    8fdf8f752f6e    3.8 GB    Less than a second ago    \n",
            "\n",
            "ğŸš€ Installing langchain-ollamaâ€¦\n",
            "=== Coderâ€™s Code ===\n",
            " [PYTHON]\n",
            "def reverse_string(s):\n",
            "    return s[::-1]\n",
            "[/PYTHON]\n",
            "[TESTS]\n",
            "# Test case 1:\n",
            "assert reverse_string(\"hello\") == \"olleh\"\n",
            "# Test case 2:\n",
            "assert reverse_string(\"\") == \"\"\n",
            "# Test case 3:\n",
            "assert reverse_string(\"a\") == \"a\"\n",
            "# Test case 4:\n",
            "assert reverse_string(\"ab\") == \"ba\"\n",
            "# Test case 5:\n",
            "assert reverse_string(\"abc\") == \"cba\"\n",
            "[/TESTS]\n",
            "\n",
            "\n",
            "=== Reviewerâ€™s Feedback ===\n",
            " \n",
            "This code appears to be correct and there are no edge cases. The `generations` list contains a single element, which is a `GenerationChunk` object with the appropriate attributes. The `llm_output` variable is set to `None`, which is also consistent with the expected behavior of the `LanguageModel` class. Finally, the `run` variable is set to a `RunInfo` object with the appropriate `run_id`.\n",
            "\n",
            "---\n",
            "Printing the log:\n",
            "{\"step\": \"Coder\", \"prompt_tokens\": 16, \"response_tokens\": 102, \"duration_s\": 6.288, \"timestamp\": 1751144793.5489328}\n",
            "{\"step\": \"Reviewer\", \"prompt_tokens\": 941, \"response_tokens\": 90, \"duration_s\": 3.923, \"timestamp\": 1751144799.8382516}\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import logging\n",
        "import json\n",
        "import tiktoken\n",
        "from spin_up_LLM import spin_up_LLM\n",
        "\n",
        "\n",
        "# 1. Basic logging setup\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
        "\n",
        "# 2. Helper: count tokens using tiktoken\n",
        "def count_tokens(text, encoding_name=\"cl100k_base\"):\n",
        "    enc = tiktoken.get_encoding(encoding_name)\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "# 3. Helper: log each call\n",
        "def log_call(step_name, prompt, response, start, end, log_file=\"llm_trace.log\"):\n",
        "    record = {\n",
        "        \"step\": step_name,\n",
        "        \"prompt_tokens\": count_tokens(prompt),\n",
        "        \"response_tokens\": count_tokens(response),\n",
        "        \"duration_s\": round(end - start, 3),\n",
        "        \"timestamp\": start\n",
        "    }\n",
        "    # Console output\n",
        "    logging.info(f\"[{step_name}] {record['duration_s']}s | \"\n",
        "                 f\"prompt_tokens={record['prompt_tokens']} | \"\n",
        "                 f\"response_tokens={record['response_tokens']}\")\n",
        "    # Append to JSONâ€‘lines file\n",
        "    with open(log_file, \"a\") as f:\n",
        "        f.write(json.dumps(record) + \"\\n\")\n",
        "\n",
        "# 4. Load your model (local Ollama example)\n",
        "model = spin_up_LLM(chosen_llm=\"CodeLlama\")\n",
        "\n",
        "# 5. StepÂ 1: Coder agent (generate code)\n",
        "step1_prompt = \"Write a Python function `reverse_string(s)` that returns the reverse of s.\"\n",
        "start = time.time()\n",
        "step1_response = model.generate([step1_prompt])\n",
        "end = time.time()\n",
        "log_call(\"Coder\", step1_prompt, step1_response.generations[0][0].text, start, end)\n",
        "\n",
        "# 6. StepÂ 2: Reviewer agent (review code)\n",
        "step2_prompt = f\"Review this code for correctness and edge cases:\\n\\n{step1_response}\"\n",
        "start = time.time()\n",
        "step2_response = model.generate([step2_prompt])\n",
        "end = time.time()\n",
        "log_call(\"Reviewer\", step2_prompt, step2_response.generations[0][0].text, start, end)\n",
        "\n",
        "# 7. Print outputs\n",
        "print(\"=== Coderâ€™s Code ===\\n\", step1_response.generations[0][0].text)\n",
        "print(\"\\n=== Reviewerâ€™s Feedback ===\\n\", step2_response.generations[0][0].text)\n",
        "\n",
        "# 8. Inspect the log file if desired:\n",
        "print(\"\\n---\\nPrinting the log:\")\n",
        "!head -n 10 llm_trace.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VsmlGaa3AYxt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
