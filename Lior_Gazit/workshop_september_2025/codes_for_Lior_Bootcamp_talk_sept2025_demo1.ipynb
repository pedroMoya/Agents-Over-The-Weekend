{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyeCwDW69Zf3"
      },
      "source": [
        "# Demo 1 - RAG Pipeline with Chained Prompt Processing\n",
        "By: [Lior Gazit](https://github.com/LiorGazit).  \n",
        "Repo: [Agents-Over-The-Weekend](https://github.com/PacktPublishing/Agents-Over-The-Weekend/tree/main/Lior_Gazit/workshop_september_2025/)  \n",
        "Running LLMs locally for free: This code leverages [`LLMPop`](https://pypi.org/project/llmpop/) that is dedicated to spinning up local or remote LLMs in a unified and modular syntax.  \n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Agents-Over-The-Weekend/blob/main/Lior_Gazit/workshop_september_2025/codes_for_Lior_Bootcamp_talk_sept2025_demo1.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a> (pick a GPU Colab session for fastest computing)  \n",
        "\n",
        "```\n",
        "Disclaimer: The content and ideas presented in this notebook are solely those of the author, Lior Gazit, and do not represent the views or intellectual property of the author's employer.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn9qIRLS9Zf7"
      },
      "source": [
        "Installing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMpPyY829Zf9",
        "outputId": "ab8f4223-f9bd-483a-de03-a7becd88e22f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip -q install llmpop\n",
        "%pip -q install sentence-transformers faiss-cpu langchain langchain_core # tiktoken langsmith langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uU75SUy9ZgA"
      },
      "source": [
        "**Imports:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MVDmCWnO9ZgB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb3_eQNB9ZgC"
      },
      "source": [
        "Setting up the Vector Store and RAG pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542,
          "referenced_widgets": [
            "eabcd6d866714eb289cb356a7f7361c5",
            "3ac014f2afba4fe8851f331dfc4dd4f1",
            "1e8d603694454c95ba8d3cafdd92174f",
            "30dc4f695d01432dbf4d86861aa75c27",
            "50fd12b047ec4a47a2039fda91ac006f",
            "8d7e146167c24e10ada24eb291d2966d",
            "f31434fa70ea4245b956e0b1e746c407",
            "bfd5958246164c1e8c18eb155f608a03",
            "7c2dbf487c424c7da1f994a5ef77b2cf",
            "b35aaac60d2749e2bd56101c3da9c34d",
            "3b62c12f8723412e9b2a8cd92400eac5",
            "c6a24b0bb7214549bee348177d58eef1",
            "679dc92a32924c95851c7f4ec0f4d34f",
            "c4b93218050d43ef8f2437057a830058",
            "376ba920ff8b4f6e9cafe0c38fef5c47",
            "c393781e197c42728aa3661229229254",
            "241c3a1461f24f04b5f09ae94e6e7a11",
            "372df47a87984f96872d6468b615e71c",
            "2f12eb9f441e4edea5db758b098af93c",
            "96a343150b9e42738ce3916a09b47487",
            "5b9722f1eccf451ba9a338c76f343275",
            "e014dee625e348ff85e5b967307ab82c",
            "541c2acd28a4473a8d1b2031120d794a",
            "c24545cf1fd54d3aaeaeb580d013278e",
            "51713fa9b3904a4fa058032788f2512b",
            "c98bb13384434d67aa45d7be226bb710",
            "8d66b77aea014870bd4d1e15366898c1",
            "98fe32c8931f4b609be009a96536a796",
            "c1cf03f3b41746988fec3550d58a7bb8",
            "973e8b2cdf0a4ac9b980f04892d56fd6",
            "224d3e7016534778a906fb05fb28e278",
            "d89e4f09adf84385a9cd0485e737c7ee",
            "cd28d3c6f60142e896651db93a768d6e",
            "06f26edf405c4c5f9d309df20e16668d",
            "8b06ab20ff444f09a50edec10edc7264",
            "c6b7b5d2c49f4c80b93980b95e1d4304",
            "fb849d166a4d4852afcd3f69a451202c",
            "937c5307772d4281bf132909d9e8c1d7",
            "43b61cc8e8b04f408ad357501ff07cb5",
            "fea0764fcb8b4b3ca9889f12b8426752",
            "72edd6a369884e2f9f6ae3a517ad9591",
            "3a6d7541ebb643c3bd082c2ec1ee7cc5",
            "e68102f06c994fc389fe6f9d67d1fac9",
            "50743a3618514376bc847bbdba01c262",
            "7517f8380041412e91ffa5651b5e163d",
            "d2199d8373d5476096c1577e851df247",
            "ab7c19c4021f4c63be803c033e334c5a",
            "01b17d54534e4b9d89c05b53c0c5f15b",
            "3ababe52b68a47feb25129d302adfed3",
            "f50af36d75d344c3996f5bc3de67ab11",
            "67766ed18e6a42e6b623ebc0bfca37b6",
            "1643e9659bfb41a4893c121bd8e4fbea",
            "535a2b58c2924b35b7273d5d01c17df4",
            "0ba4cacd18e54b789e15ea3d17f3d5e8",
            "905a3c3a51464681aaec0d3ad0847bcd",
            "becf05d54a49423ebd18864a7b6730db",
            "36ed05cc00c64559a97b2e4ceaae29ac",
            "aa953361e1f74fedb282b67506e92c73",
            "cc9718f332824e2eaa46a41acf92a178",
            "a74fa1a6665449dd9ec41b5c57651c47",
            "e3304cf7066143f3bc832d4ac6d1d6a4",
            "ffa3df92f6e14a338d3357ad6999e5ac",
            "bfa4fd7f13ab476d9d6aa5654d562376",
            "7dbbadfc203647a1bbddc8a0c80be985",
            "72219362144e43a0802ff7c6074a83f7",
            "787e01a757334147921aeaa0f4c9e1b3",
            "b9a67dd97290431697c76d4e69ef7087",
            "8f1187ae4cf34181b4a492dabd9dae5e",
            "4fc515da83a048d28bf8bd112fc0de73",
            "bfd8572c0bb1469eba930f68d9ad3fca",
            "c154b5771cfe4331919973938b4b184f",
            "4e481e24b2e14a63ace1511867b122af",
            "07a9600a6b844c879ae284db2dee979b",
            "a468a1b3e11e4bf4892dc03a30367936",
            "d904bf0f39c34cc4b4cc347f8cb3633e",
            "81e7717c89eb40fb9a37fd76d0cfe5b7",
            "70750c5f598f4e4db1632d972af8e622",
            "43969edb2e8c4f2487d62915097be66f",
            "336f27ce62e54524ba752375dfb08085",
            "1e89fa14ff474fd9a0233ecfc76653be",
            "1c858329484e41fe8e8f3b0232a7516c",
            "a9a7cc60a0f943dfa055cb385f909789",
            "4306293aea084c8996625c7b68f7fa90",
            "bf902a92fc0b43fcae6a2733454d4624",
            "cde0ebff24ac4fa1920cf386b3b8fe51",
            "196f9c18da144d06937e1d68abe90356",
            "6cfb911d7a7a439ab27034053e63d376",
            "3c24b54244c04c78aab238f1af444235",
            "c2d9d84a987a4e6a9e029b5abde976b6",
            "05e591c486a2442dbf626bcba02b0d7f",
            "8ed0ce40cb2f4a12acd3675b71385ec5",
            "6681dfc3a8d64ecdaec3658c9bca6aa3",
            "6cc5df914e2d402599e85c9d7d684995",
            "d3d0036fc7c0405384d509589e6771e0",
            "b118456f19e44ebeba0a88121727be5e",
            "a4927b00501c4b4fb56a1c9d028c7f7c",
            "884ea63faedc4f9c95108434f41464c9",
            "84413e6d07df4e858e4e3befa7a06e58",
            "761b10470f00410f86c3507df8c2de6f",
            "23794be5133e4a088b95f990087e250b",
            "4a221b9a2d0848589ae6c9feae0b445c",
            "65bd9c37b089424897cb7bfac2be7aae",
            "4630d19977f64a58a54925f1242bc8b2",
            "e5ba765424db492cb96660f5d7be8a27",
            "b8e78c3e3ff74dc1ba091f7482430092",
            "6bdfb081c97648a5bf552609d5c963ec",
            "da2fa3a791b14534a05689853c7a06c0",
            "9c280ad675ef4f0780dad62d1604f0d4",
            "5019c200affb42649d73b609581ad2f3",
            "627e7c2c343d4d8a926ef66794460369",
            "901c1a19ec6d485bbfefb15334294213",
            "68b894a0e1a74d899ad11408180d5055",
            "9361af0f4c7242afb058269aacf07a31",
            "1b2696dbc306460eabd43f63d59ed14c",
            "507ce82ebb464df8852d54fd822e057c",
            "99fd12da400d4bc0899e2a1dfad713b0",
            "716d9560eea34dd5b71afabb692c7fa1",
            "f16c1efc3a71447586e22570708902a3",
            "a1ef4ac445ea496b828d0631af92f65f",
            "94aaf6da48d946d3b84010f4c0233db5",
            "0440215b08d14a55acd5071a7dd808aa"
          ]
        },
        "id": "EEqyCnZQ9ZgD",
        "outputId": "93d703eb-6262-439d-cefe-f7a4e8098a49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eabcd6d866714eb289cb356a7f7361c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6a24b0bb7214549bee348177d58eef1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "541c2acd28a4473a8d1b2031120d794a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06f26edf405c4c5f9d309df20e16668d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7517f8380041412e91ffa5651b5e163d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "becf05d54a49423ebd18864a7b6730db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9a67dd97290431697c76d4e69ef7087",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43969edb2e8c4f2487d62915097be66f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2d9d84a987a4e6a9e029b5abde976b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23794be5133e4a088b95f990087e250b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "901c1a19ec6d485bbfefb15334294213",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Retrieved documents for context:\n",
            " [\"The company's earnings call mentioned concerns over increased production costs.\", 'Recent financial filings show revenue growth despite supply chain issues.']\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Example documents (could be clinical notes, financial filings, etc.)\n",
        "documents = [\n",
        "    \"Patient has diabetes type 2 and shows high glucose levels.\",\n",
        "    \"Recent financial filings show revenue growth despite supply chain issues.\",\n",
        "    \"Patient diagnosed with hypertension, recommended lifestyle changes.\",\n",
        "    \"The company's earnings call mentioned concerns over increased production costs.\",\n",
        "]\n",
        "\n",
        "# Step 1: Create embeddings using SentenceTransformer\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # lightweight embedding model\n",
        "\n",
        "# Generate embeddings for the documents\n",
        "document_embeddings = embedding_model.encode(documents)\n",
        "\n",
        "# Step 2: Setup FAISS vector store\n",
        "dimension = document_embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "faiss_index.add(document_embeddings)\n",
        "\n",
        "# Step 3: Define the retriever(query) function\n",
        "def retriever(query, top_k=2):\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "\n",
        "    # Perform the similarity search in the FAISS index\n",
        "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
        "\n",
        "    # Retrieve the top_k most similar documents\n",
        "    retrieved_docs = [documents[idx] for idx in indices[0]]\n",
        "\n",
        "    return retrieved_docs\n",
        "\n",
        "# Example usage of the retriever\n",
        "query = \"What did the company say about production costs?\"\n",
        "context_docs = retriever(query)\n",
        "print(\"\\n\\nRetrieved documents for context:\\n\", context_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpsLTVeT9ZgE"
      },
      "source": [
        "Prompting using a locally hosted LLM via Ollama:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hdYXyYZ9ZgF",
        "outputId": "42822353-3d21-4dd4-e168-1182e0996ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Installing Ollama...\n",
            "üöÄ Starting Ollama server...\n",
            "‚Üí Ollama PID: 2728\n",
            "‚è≥ Waiting for Ollama to be ready‚Ä¶\n",
            "Ready!\n",
            "\n",
            "üöÄ Pulling model 'llama3.2:1b'‚Ä¶\n",
            "All done setting up Ollama (ChatOllama).\n",
            "\n",
            "Given the context provided, it appears that the patient has both diabetes type 2 and hypertension (high blood pressure). The first note mentions the high glucose levels, which are a key indicator of diabetes type 2. However, the second note specifically mentions hypertension as part of the recommended lifestyle changes for the patient. This suggests that while the patient may have diabetes type 2, they also have hypertension, possibly indicating that their overall health management plan will focus on both conditions.\n"
          ]
        }
      ],
      "source": [
        "from llmpop import init_llm\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "query = \"What is the patient's diagnosis given these notes?\"\n",
        "context_docs = retriever(query)\n",
        "question = f\"Using the following context, answer the question:\\n\\n{context_docs}\\n\\nQ: {query}\\n\\n---\\nA:\"\n",
        "local_llm = init_llm(model=\"llama3.2:1b\", provider=\"ollama\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"Q: {question}\\nA:\")\n",
        "print((prompt | local_llm).invoke({\"question\":question}).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK1oYpXy9ZgG"
      },
      "source": [
        "Prompting using OpenAI's API (paid) route:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv5oEytT9ZgG",
        "outputId": "d4953a77-0c9b-436d-9a90-426a93486598"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "\n",
            "\n",
            "\n",
            "The patient has been diagnosed with diabetes type 2 and hypertension.\n"
          ]
        }
      ],
      "source": [
        "# In Colab, use getpass to securely prompt for your API key\n",
        "from getpass import getpass\n",
        "import openai\n",
        "\n",
        "openai.api_key = getpass(\"Paste your OpenAI API key: \")\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\":\"system\",\"content\":\"You are a medical assistant.\"},\n",
        "        {\"role\":\"user\",  \"content\": question}\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_api = response.choices[0].message.content\n",
        "print(\"\\n\\n\")\n",
        "print(answer_api)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
