{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedroMoya/Agents-Over-The-Weekend/blob/main/Lior_Gazit/workshop_september_2025/codes_for_Lior_Bootcamp_talk_sept2025_demo1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyeCwDW69Zf3"
      },
      "source": [
        "# Demo 1 - RAG Pipeline with Chained Prompt Processing\n",
        "By: [Lior Gazit](https://github.com/LiorGazit).  \n",
        "Repo: [Agents-Over-The-Weekend](https://github.com/PacktPublishing/Agents-Over-The-Weekend/tree/main/Lior_Gazit/workshop_september_2025/)  \n",
        "Running LLMs locally for free: This code leverages [`LLMPop`](https://pypi.org/project/llmpop/) that is dedicated to spinning up local or remote LLMs in a unified and modular syntax.  \n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Agents-Over-The-Weekend/blob/main/Lior_Gazit/workshop_september_2025/codes_for_Lior_Bootcamp_talk_sept2025_demo1.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a> (pick a GPU Colab session for fastest computing)  \n",
        "\n",
        "```\n",
        "Disclaimer: The content and ideas presented in this notebook are solely those of the author, Lior Gazit, and do not represent the views or intellectual property of the author's employer.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn9qIRLS9Zf7"
      },
      "source": [
        "Installing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NMpPyY829Zf9"
      },
      "outputs": [],
      "source": [
        "%pip -q install llmpop\n",
        "%pip -q install sentence-transformers faiss-cpu langchain langchain_core # tiktoken langsmith langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uU75SUy9ZgA"
      },
      "source": [
        "**Imports:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MVDmCWnO9ZgB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb3_eQNB9ZgC"
      },
      "source": [
        "Setting up the Vector Store and RAG pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEqyCnZQ9ZgD",
        "outputId": "a71a27e6-110a-4fcd-fcf4-1e9b49e632bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Retrieved documents for context:\n",
            " [\"The company's earnings call mentioned concerns over increased production costs.\", 'Recent financial filings show revenue growth despite supply chain issues.']\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Example documents (could be clinical notes, financial filings, etc.)\n",
        "documents = [\n",
        "    \"Patient has diabetes type 2 and shows high glucose levels.\",\n",
        "    \"Recent financial filings show revenue growth despite supply chain issues.\",\n",
        "    \"Patient diagnosed with hypertension, recommended lifestyle changes.\",\n",
        "    \"The company's earnings call mentioned concerns over increased production costs.\",\n",
        "]\n",
        "\n",
        "# Step 1: Create embeddings using SentenceTransformer\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # lightweight embedding model\n",
        "\n",
        "# Generate embeddings for the documents\n",
        "document_embeddings = embedding_model.encode(documents)\n",
        "\n",
        "# Step 2: Setup FAISS vector store\n",
        "dimension = document_embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "faiss_index.add(document_embeddings)\n",
        "\n",
        "# Step 3: Define the retriever(query) function\n",
        "def retriever(query, top_k=2):\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "\n",
        "    # Perform the similarity search in the FAISS index\n",
        "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
        "\n",
        "    # Retrieve the top_k most similar documents\n",
        "    retrieved_docs = [documents[idx] for idx in indices[0]]\n",
        "\n",
        "    return retrieved_docs\n",
        "\n",
        "# Example usage of the retriever\n",
        "query = \"What did the company say about production costs?\"\n",
        "context_docs = retriever(query)\n",
        "print(\"\\n\\nRetrieved documents for context:\\n\", context_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpsLTVeT9ZgE"
      },
      "source": [
        "Prompting using a locally hosted LLM via Ollama:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hdYXyYZ9ZgF",
        "outputId": "5a379bcd-868f-445a-8fc6-ab5a7666c50a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Ollama server...\n",
            "‚Üí Ollama PID: 25002\n",
            "‚è≥ Waiting for Ollama to be ready‚Ä¶\n",
            "Ready!\n",
            "\n",
            "üöÄ Pulling model 'llama3.2:1b'‚Ä¶\n",
            "All done setting up Ollama (ChatOllama).\n",
            "\n",
            "Based on the provided context, it appears that the patient's primary diagnosis is insulin resistance or type 2 diabetes. The mention of \"high glucose levels\" supports this diagnosis, as people with type 2 diabetes typically have elevated blood sugar readings.\n",
            "\n",
            "The additional note about hypertension (high blood pressure) is also relevant to the patient's overall health, but it does not directly indicate a specific diagnosis related to their primary condition (diabetes). It may be worth considering in conjunction with the diabetes diagnosis or referring the patient for further testing to rule out other conditions.\n"
          ]
        }
      ],
      "source": [
        "from llmpop import init_llm\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "query = \"What is the patient's diagnosis given these notes?\"\n",
        "context_docs = retriever(query)\n",
        "question = f\"Using the following context, answer the question:\\n\\n{context_docs}\\n\\nQ: {query}\\n\\n---\\nA:\"\n",
        "local_llm = init_llm(model=\"llama3.2:1b\", provider=\"ollama\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"Q: {question}\\nA:\")\n",
        "print((prompt | local_llm).invoke({\"question\":question}).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK1oYpXy9ZgG"
      },
      "source": [
        "Prompting using OpenAI's API (paid) route:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv5oEytT9ZgG",
        "outputId": "2680489f-bc49-455a-b87a-ffebd1541db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "\n",
            "\n",
            "\n",
            "The patient's diagnoses are diabetes type 2 with high glucose levels and hypertension.\n"
          ]
        }
      ],
      "source": [
        "# In Colab, use getpass to securely prompt for your API key\n",
        "from getpass import getpass\n",
        "import openai\n",
        "\n",
        "openai.api_key = getpass(\"Paste your OpenAI API key: \")\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\":\"system\",\"content\":\"You are a medical assistant.\"},\n",
        "        {\"role\":\"user\",  \"content\": question}\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_api = response.choices[0].message.content\n",
        "print(\"\\n\\n\")\n",
        "print(answer_api)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}